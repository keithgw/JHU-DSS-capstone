rbinom(10, size = 2, prob = .5)
rbinom(20, size = 2, prob = .5)
rbinom(20, size = 1, prob = .5)
ctwitter <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"data", "final", "en_US", "en_US.twitter.txt")
cblogs <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"data", "final", "en_US", "en_US.blogs.txt")
cnews <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"data", "final", "en_US", "en_US.news.txt")
### Read in files
corpus_t <- readLines(ctwitter, encoding = "UTF-8", skipNul=TRUE)
corpus_b <- readLines(cblogs, encoding = "UTF-8", skipNul=TRUE)
corpus_n <- readLines(cnews, encoding = "UTF-8", skipNul=TRUE)
corpus_sample <- function(corpus, p) {
mask <- rbinom(length(corpus), size=1, prob=p)
mask <- mask == 1
return(corpus[mask])
}
corpus_n <- readLines(file(cnews, "rb"), encoding = "UTF-8", skipNul=TRUE)
set.seed(13720)
sample_proportion = 0.06
sample_twitter <- corpus_sample(corpus_t, p=sample_proportion)
sample_blog <- corpus_sample(corpus_b, p=sample_proportion)
sample_news <- corpus_sample(corpus_n, p=sample_proportion)
scfile <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"JHU-DSS-capstone", "data", "sample_corpus_06.txt")
lapply(list(sample_twitter, sample_blog, sample_news),
function(x){write(x, file=scfile, append=TRUE)})
library(tm)      # for text mining
library(dplyr)   # for data manipulation
library(stringr) # for reg exp, string replacement
library(stringi)
library(RWeka)   # for NLP, n-gram tokenizer
library(slam)    # for calcualtions on simple triplet matrices (outputs of tm)
library(data.table) # fast search
scfile <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"JHU-DSS-capstone", "data", "sample", "sample_corpus_06.txt")
sample_corpus <- readLines(scfile)
sample_corpus <- readLines(file(scfile, "rb"), encoding = "UTF-8", skipNul=TRUE)
?write
train_file <- file.path("~", "Data Science Coursera", "capstone-JHU_swiftkey",
"JHU-DSS-capston", "data", "sample", "train",
"train_corpus.txt")
test_file <- file.path("~", "Data Science Coursera", "capstone-JHU_swiftkey",
"JHU-DSS-capston", "data", "sample", "test",
"test_corpus.txt")
mask <- rbinom(length(sample_corpus), size=1, prob=2/3)
set.seed(2012)
mask <- rbinom(length(sample_corpus), size=1, prob=2/3)
in_train <- mask == 1
write(sample_corpus[in_train], train_file)
training <- sample_corpus[in_train]
testing <- sample_corpus[-in_train]
str(training)
str(testing)
len(training)
length(training)
length(testing)
length(sample_corpus)
testing <- sample_corpus[!in_train]
length(testing)
length(training) + length(testing)
length(sample_corpus)
write(training, train_file)
?write
write(training, file=train_file)
train_file
write(training, file=train_file, append=TRUE)
View(training)
summary(training)
head(training)
?writeLines
writeLines(training, file=train_file)
writeLines(training, con=train_file)
train_file <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"JHU-DSS-capstone", "data", "sample", "train",
"train_corpus.txt")
test_file <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey",
"JHU-DSS-capstone", "data", "sample", "test",
"test_corpus.txt")
write(training, file=train_file)
write(training, file=train_file)
write(testing, file=test_file)
dtm <- readRDS("dtm.rds")
head(dtm)
summary(dtm)
type(dtm)
class(dtm)
str(dtm)
head(dtm$Terms)
head(dtm$j)
head(dtm$v)
dtm$dimnames
head(dtm$dimnames)
head(dtm$dimnames[2])
str(dtm$dimnames)
str(dtm$dinames[1])
str(dtm$dimnames[1])
str(dtm$dimnames[2])
head(dtm$dimnames[2][1])
head(dtm$dimnames[2][[1]])
uniques <- dtm$dimnames[2][[1]][dtm$v == 1]
length(uniques)
head(uniques)
str(uniques)
for(i in 1:10){print(c(i, uniques[i]))}
scorp <- Corpus(DirSource(file.path("~", "Data Science Coursera",
"capstone-JHU-swiftkey", "JHU-DSS-capstone",
"data", "sample", "train")))
scorp <- Corpus(DirSource(file.path("~", "Data Science Coursera",
"capstone-JHU-swiftkey", "JHU-DSS-capstone",
"data", "sample", "train")))
to_space <- content_transformer(function(x, pattern)
str_replace_all(x, pattern, " "))
# replace /, @, |, -,  with a space
corp <- tm_map(corp, to_space, "/|@|\\||-")
# custom function to remove hashtags
rmv_hashtags <- content_transformer(function(x)
str_replace_all(x, "#.* |#.*$",""))
corp <- tm_map(corp, rmv_hashtags)
corp <- tm_map(corp, to_space, "/|@|\\||-")
rm(scorp)
corp <- Corpus(DirSource(file.path("~", "Data Science Coursera",
"capstone-JHU-swiftkey", "JHU-DSS-capstone",
"data", "sample", "train")))
corp <- tm_map(corp, to_space, "/|@|\\||-")
rmv_hashtags <- content_transformer(function(x)
str_replace_all(x, "#.* |#.*$",""))
corp <- tm_map(corp, rmv_hashtags)
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, to_space, "[^A-Za-z]")
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, PlainTextDocument)
dtm <- DocumentTermMatrix(sorp, control=list(wordLengths=c(0,Inf)))
library(tm)      # for text mining
library(ggplot2) # for plotting
library(dplyr)   # for data manipulation
library(stringr) # for reg exp, string replacement
library(stringi)
library(RWeka)   # for NLP, n-gram tokenizer
library(slam)    # for calcualtions on simple triplet matrices (outputs of tm)
library(data.table) # fast search
dtm <- DocumentTermMatrix(corp, control=list(wordLengths=c(0,Inf)))
uniques <- dtm$dimnames[2][[1]][dtm$v == 1]
str(uniques)
dtm$dimnames[2][[1]][nchar(dtm$dimnames[2][[1]]) < 3]
?str_replace_all
sapply(uniques, function(pattern){str_replace_all(corp, pattern, '<unk>')})
uniques[1]
class(uniques[1])
?check_string
class(corp)
inspect(corp)
str(corp[1])
str(corp[1][[1]])
str(corp[1][[1]][[1]])
sapply(uniques, function(pattern){str_replace(corp[1][[1]][[1]], pattern, '<unk>')})
?str_replace
test <- c('one', 'two', 'three')
sapply(test, function(x){str_con(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
sapply(test, function(x){str_c(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
test2 <- sapply(test, function(x){str_c(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
paragraph <- 'one is the most exciting two I've ever three'
paragraph <- 'one is the most exciting two I have ever three'
sapply(test2, function(pattern){str_replace(paragraph, pattern '<unk>')})
sapply(test2, function(pattern){str_replace(paragraph, pattern, '<unk>')})
paragraph <- sapply(test2, function(pattern){str_replace(paragraph, pattern, '<unk>')})
paragraph
paragraph
test2 <- lapply(test, function(x){str_c(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
test2
test
test2 <- apply(test, function(x){str_c(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
test2 <- sapply(test, function(x){str_c(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
test2
class(test2)
test2[1]
length(test2)
patterns <- sapply(uniques,
function(x){str_c(' ', x, ' |', '^', x, ' |', ' ', x, '$')})
to_unk <- content_transformer(function(x, pattern)
str_replace(x, pattern, " <unk> "))
for (term in test){str_c('check', term)}
test
for (term in test){test <- str_c('check', term)}
test
test <- c('one', 'two', 'three')
for (term in test){print(term)}
for (term in patterns){
corp <- tm_map(corp, to_unk, term)
}
corp <- tm_map(corp, stripWhitespace)
smoothed_dtm <- DocumentTermMatrix(corp, control=list(wordLengths=c(0,Inf)))
max(smoothed_dtm)
freq_sorted <- function(ngram) {
return(sort(col_sums(ngram), decreasing=TRUE))
}
head(freq_sorted(smoothed_dtm))
head(freq_sorted(smoothed_dtm), 100)
head(freq_sorted(smoothed_dtm), 200)
smoothed_dtm[names(smoothed_dtm)=='<unk>']
smoothed_dtm[str_detect(names(smoothed_dtm), '<unk>')]
smoothed_dtm[str_detect(names(freq_sorted(smoothed_dtm)), '<unk>')]
corp[1][[1]][[1]][1:10]
corp[1][[1]][[1]][1:20]
corp[1][[1]][[1]][20:40]
saveRDS(smoothed_dtm, "smoothed_dtm.rds")
saveRDS(corp, 'corpus_unk.rds')
count_ngrams <- function(n) {
options(mc.cores = 2)
ctrl <- Weka_control(min = n, max = n)
ngram_tokenizer <- function(x) NGramTokenizer(x, control = ctrl)
dtm_ngram <- DocumentTermMatrix(corp,
control = list(tokenize = ngram_tokenizer))
return(dtm_ngram)
}
bigrams <- count_ngrams(2)
trigrams <- count_ngrams(3)
bisorted <- freq_sorted(bigrams)
trisorted <- freq_sorted(trigrams)
bidat <- data.table(ngram = names(bisorted), frequency = bisorted)
tridat <- data.table(ngram = names(trisorted), frequency = trisorted)
head(bisorted)
head(bisorted, 100)
saveRDS(bidat, "bidat_unk.rds")
rm(bigrams)
rm(bidat)
rm(bisorted)
rm(dtm)
rm(smoothed_dtm)
rm(patterns)
rm(uniques)
trigrams <- count_ngrams(3)
count_ngrams <- function(n) {
options(mc.cores = 1)
ctrl <- Weka_control(min = n, max = n)
ngram_tokenizer <- function(x) NGramTokenizer(x, control = ctrl)
dtm_ngram <- DocumentTermMatrix(corp,
control = list(tokenize = ngram_tokenizer))
return(dtm_ngram)
}
trigrams <- count_ngrams(3)
bidat <- readRDS("bidat_unk.rds")
str(bidat)
count_ngrams <- function(n) {
options(mc.cores = 2)
ctrl <- Weka_control(min = n, max = n)
ngram_tokenizer <- function(x) NGramTokenizer(x, control = ctrl)
dtm_ngram <- DocumentTermMatrix(corp,
control = list(tokenize = ngram_tokenizer))
return(dtm_ngram)
}
trigrams <- count_ngrams(3)
head(bidat$ngram, 100)
test <- head(bidat$ngram, 100)
test
class(test)
length(test)
?set
set(1, 1, 2, 3, 4, 4, 5)
unique(list(1, 1, 2, 3))
unique(c(1, 1, 2, 3))
unique(c('a', 'a', 'b', 'c'))
class(unique(c('a', 'a', 'b', 'c')))
str_split(test, " ")
test_dt <- data.table(ngram = test)
test_dt
library(dplyr)
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[1])
test_dt
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[[1]])
test_dt
test_dt <- mutate(test_dt, first = str_split(ngram, " "))
test_dt
test_dt$first[1]
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[1][1])
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[[1]][1])
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[1][[1]])
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[1][[1]][1])
test_dt <- data.table(ngram = test)
test_dt <- mutate(test_dt, first = str_split(ngram, " "))
test_dt$first[1]
test_dt$first[1][[1]][1]
test_dt$first[1][[1]][1][1]
test_dt$first[1][[1]][1][1][1]
test_dt$first[1][[1]]
test_dt$first[1][[1]][1]
class(test_dt$first[1][[1]][1])
test_dt <- mutate(test_dt, first = str_split(ngram, " ")[1][[1]][1])
test_dt
?str_extract
str_extract(test$ngram, "^*. ")
test_dt <- mutate(test_dt, first = str_extract(ngram, "^*. ")
)
test_dt
test_dt <- mutate(test_dt, first = str_extract(ngram, "^.* ")
)
test_dt
bidat <- bidat %>% mutate(first = str_extract(ngram, "^.* "))
head(bidat)
first_words <- unique(bidat$first)
length(first_words)
class(first_words)
select(test, first)
select(test_dt, first)
prediction_bigrams <- data.table(ngram = character(), frequency = integer())
?rbind
test_dt
head(bidat)
test_dt <- head(bidat, 100)
test_dt
filter(test_dt, first == "i")
filter(test_dt, first == i)
filter(test_dt, first == 'i')
test_dt
filter(test_dt, first == 'a')
test_dt[test_dt$first == "i"]
class(test_dt$first)
class(test_dt$first[1])
test_dt[str_detect(test_dt$first), "i"]
test_dt[str_detect(test_dt$first, "i")]
test_dt[str_detect(test_dt$first, "^i$")]
test_dt[str_detect(test_dt$first, "^i")]
test_dt[str_detect(test_dt$first, "^i&$i")]
test_dt[str_detect(test_dt$first, "^i&i$")]
test_dt[str_detect(test_dt$first, "i$")]
test_dt[str_detect(test_dt$first, "$i")]
filter(test_dt, test_dt$first == "i")
?first
?filter
test_dt$first[1] == "in"
test_dt$first[1] == character('in')
test_dt$first[1] == 'in'
test_dt$first[1]
test_dt$first[1] == 'of'
test_dt$first[1] == 'of '
unique_firsts[1]
unique_firsts <- unique(bidat$first)
unique_firsts[1]
filter(test_dt, first == "of ")
max(filter(test_dt, first == "of ")$frequency)
test_of <- filter(test_dt, first == "of ")
test_of
filter(test_of, frequency == max(frequency))
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram)
prediction_bigrams <- data.table(ngram = character())
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram) %>% rbind(prediciton_bigrams)
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram) %>% rbind(prediction_bigrams)
prediction_bigrams
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram) %>% rbind(, prediction_bigrams)
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram) %>% rbind(prediction_bigrams)
prediction_bigrams
?data.table
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram) %>% predition_bigrams[]
prediction_bigrams[filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram)]
prediction_bigrams
prediction_bigrams[filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram), ngram]
rbind(prediction_bigrams, filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
prediction_bigrams
rbind(prediction_bigrams, data.table(ngram = c('of the')))
prediction_bigrams
prediction_bigrams <- filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram) %>% rbind(prediction_bigrams)
prediction_bigrams
unique_firsts <- unique(bidat$first)
for (first_ in unique_firsts) {
## filter by first word
prediction_bigrams <- filter(bidat, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram) %>%
rbind(prediction_bigrams)
## extract the observation with max frequency
}
prediction_bigrams <- data.table(ngram = character())
for (first_ in unique_firsts) {
## filter by first word
prediction_bigrams <- filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram) %>%
rbind(prediction_bigrams)
}
unique_firsts <- unique(test_dt$first)
for (first_ in unique_firsts) {
## filter by first word
prediction_bigrams <- filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram) %>%
rbind(prediction_bigrams)
}
prediction_bigrams
unique_firsts
c(prediction_bigrams$ngram, filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram)
class(filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram)[1]
filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram)
str(prediction_bigrams)
prediction_bigrams <- data.table(ngram = character())
rbind(prediction_bigrams, filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
prediction_bigrams
?rbind
prediction_bigrams <- list()
str(test_dt)
str(unique_firsts)
for (first_ in unique_firsts) {
## filter by first word
prediction_bigrams <- c(prediction_bigrams, filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
}
str(prediction_bigrams)
rbindlist(prediction_bigrams)
class(filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
str(filter(test_dt, first == "of ") %>% filter(frequency == max(frequency)) %>% select(ngram))
str(prediction_bigrams)
str(prediction_bigrams[1])
str(prediction_bigrams[[1]])
rbindlist(prediction_bigrams)
for (first_ in unique_firsts) {
## filter by first word
prediction_bigrams <- rbind(prediction_bigrams, filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
}
prediction_bigrams <- list()
for (first_ in unique_firsts) {
## filter by first word
prediction_bigrams <- c(prediction_bigrams, filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
}
str(prediction_bigrams)
summary(prediction_bigrams)
?list
prediction_bigrams <- vector("list", lenght(unique_firsts))
prediction_bigrams <- vector("list", length(unique_firsts))
prediction_bigrams
for (i in 1:length(unique_firsts)) {
## filter by first word
prediction_bigrams[[i]] <- filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
}
for (i in 1:length(unique_firsts)) {
## filter by first word
prediction_bigrams[[i]] <- filter(test_dt, first == first_) %>%
filter(frequency == max(frequency)) %>%
select(ngram)
}
str(prediction_bigrams)
rbindlist(prediction_bigrams)
prediction_bigrams <- vector("list", length(unique_firsts))
for (i in 1:length(unique_firsts)) {
## filter by first word
prediction_bigrams[[i]] <- filter(test_dt, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram)
}
rbindlist(prediction_bigrams)
unique_firsts <- unique(bidat$first)
max_bigrams <- vector("list", length(unique_firsts))
for (i in 1:length(unique_firsts)) {
## filter by first word
max_bigrams[[i]] <- filter(bidat, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram)
}
seq(1, 10)
unique_firsts <- unique(test_dt$first)
max_bigrams <- vector("list", length(unique_firsts))
lapply(seq(1, length(unique_firsts)), function(i) max_bigrams[[i]] <- filter(test_dt, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
str(max_bigrams)
max_bigrams <- lapply(seq(1, length(unique_firsts)), function(i) max_bigrams[[i]] <- filter(test_dt, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
str(max_bigrams)
unique_firsts <- unique(bidat$first)
max_bigrams <- vector("list", length(unique_firsts))
max_bigrams <- lapply(seq(1, length(unique_firsts)),
function(i) {
max_bigrams[[i]] <- filter(bidat,
first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))}
max_bigrams <- lapply(seq(1, length(unique_firsts)),
function(i)
max_bigrams[[i]] <- filter(bidat,
first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
max_bigrams <- lapply(seq(1, length(unique_firsts)),
function(i) max_bigrams[[i]] <- filter(bidat,
first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
max_bigrams <- lapply(seq(1, length(unique_firsts)),
function(i) max_bigrams[[i]] <- filter(bidat, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
max_bigrams <- lapply(seq(1, length(unique_firsts)),
function(i) max_bigrams[[i]] <- filter(bidat, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram)))
max_bigrams <- lapply(seq(1, length(unique_firsts)),
function(i) max_bigrams[[i]] <- filter(bidat, first == unique_firsts[i]) %>%
filter(frequency == max(frequency)) %>%
select(ngram))
shiny::runApp()
