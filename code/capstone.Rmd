---
title: "Raw Corpora EDA - Predictive Text App"
author: "Keith G. Williams"
date: "Saturday, July 11, 2015"
output: html_document
---

## Load Corpora

```{r, message=FALSE, warning=FALSE}
library(tm)      # for text mining
library(ggplot2) # for plotting
library(dplyr)   # for data manipulation
library(stringr) # for reg exp, string replacement
library(stringi)
library(RWeka)   # for NLP, n-gram tokenizer
library(slam)    # for calcualtions on simple triplet matrices (outputs of tm)
library(data.table) # fast search
```

```{r}
ctwitter <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey", "data", "final", "en_US", "en_US.twitter.txt")
cblogs <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey", "data", "final", "en_US", "en_US.blogs.txt")
cnews <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey", "data", "final", "en_US", "en_US.news.txt")

corpus_t <- readLines(ctwitter, 
                      encoding = "UTF-8", skipNul=TRUE)  # read the entire file
corpus_b <- readLines(cblogs, encoding = "UTF-8", skipNul=TRUE)
corpus_n <- readLines(file(cnews, "rb"), encoding = "UTF-8", skipNul=TRUE)
```

## Quiz questions

In the en_US.twitter.txt, find the ratio of the number of lines that contain the word "love" (all lowercase) to the number of lines that contain the word "hate" (all lowercase).
```{r}
regex_love <- "^love | love "
regex_hate <- "^hate | hate "
love_lines <- str_detect(corpus_t, regex_love)
hate_lines <- str_detect(corpus_t, regex_hate)

# ratio of lines with "love" to lines with "hate"
(lh_ratio <- sum(love_lines) / sum(hate_lines))
```

The one tweet in the en_US twitter data set containing the word "biostat" says what?
```{r}
corpus_t[which(str_detect(corpus_t, "biostat"))]
```

How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing"?
```{r}
pattern <- "A computer once beat me at chess, but it was no match for me at kickboxing"
sum(str_detect(corpus_t, pattern))
```

## EDA

### Initial Exploration

Size and structure of each corpus:
```{r}
files <- c(ctwitter, cblogs, cnews)

# Get size of files
file_sizes_mb <- sapply(seq(1:3), function(x) round(file.info(files[x])[[1]] / 1e6, 1))

# get line counts for each corpus
line_counts <- c(length(corpus_t), length(corpus_b), length(corpus_n))

# get mean word count per line for a sample from a corpus
mean_word_count <- function(corpus) {
    mean(sapply(seq(1:length(corpus)), 
               function(x) nrow(str_locate_all(corpus, "\\S+")[[x]])
               )
        )
}

# because files are so large, sampling will be used to find word counts
set.seed(2718)
nsamples <- 1000
sample_size <- 40

# find sample means
## twitter
meanwc_twitter <- numeric()
for (i in 1:nsamples) {
    meanwc_twitter <- c(meanwc_twitter, 
                        mean_word_count(sample(corpus_t, sample_size)))
}

## blogs
meanwc_blogs <- numeric()
for (i in 1:nsamples) {
    meanwc_blogs <- c(meanwc_blogs, 
                      mean_word_count(sample(corpus_b, sample_size)))
}

## news
meanwc_news <- numeric()
for (i in 1:nsamples) {
    meanwc_news <- c(meanwc_news, 
                     mean_word_count(sample(corpus_n, sample_size)))
}

# estimate mu, sigma, and uncertainty
## twitter
mu_wc_twitter <- mean(meanwc_twitter)
confint_muwc_twitter <- mu_wc_twitter + c(-1, 1) * qnorm(0.975) / sd(meanwc_twitter)
sigma_wc_twitter <- sd(meanwc_twitter) / sqrt(sample_size)

## blogs
mu_wc_blogs <- mean(meanwc_blogs)
confint_muwc_blogs <- mu_wc_blogs + c(-1, 1) * qnorm(0.975) / sd(meanwc_blogs)
sigma_wc_blogs <- sd(meanwc_blogs) / sqrt(sample_size)

## news
mu_wc_news <- mean(meanwc_news)
confint_muwc_news <- mu_wc_news + c(-1, 1) * qnorm(0.975) / sd(meanwc_news)
sigma_wc_news <- sd(meanwc_news) / sqrt(sample_size)

# estimate word counts
wc_twitter <- round(mu_wc_twitter * line_counts[1], 0)
wc_blogs <- round(mu_wc_blogs * line_counts[2], 0)
wc_news <- round(mu_wc_news * line_counts[3], 0)

# tabulate summary statistics for corpora
summary_c <- data.frame(corpus = c('twitter', 'blogs', 'news'),
                        file_size = file_sizes_mb,
                        n_lines = line_counts,
                        word_ct = c(wc_twitter, wc_blogs, wc_news),
                        avg_wpl = c(mu_wc_twitter, mu_wc_blogs, mu_wc_news),
                        sd_wpl = c(sigma_wc_twitter, 
                                   sigma_wc_blogs, 
                                   sigma_wc_news)
                        )

# write out summary_c data.frame for later use
# write.table(summary_c, file = "corpora_summary_stats.txt")
```

Initial questions to consider:  
1. What are the distributions of word frequencies?  
2. What are the frequencies of 2- and 3-grams?  
3. How many unique words are needed in a frequency sorted dictionary to cover:  
    a. 50% of the word instances?  
    b. 90%?  
4. How can one evaluate the number of words eminating from a foreign language?  
5. Are there ways to increase coverage?

Since the files are large, sampling will be used to build n-grams.  
## Build sample corpus  

```{r}
# create a logical mask for each corpus of size 4%
corpus_sample <- function(corpus, p) {
    mask <- rbinom(length(corpus), size=1, prob=p)
    mask <- mask == 1
    return(corpus[mask])
}

sample_twitter <- corpus_sample(corpus_t, p=0.04)
sample_blog <- corpus_sample(corpus_b, p=0.04)
sample_news <- corpus_sample(corpus_n, p=0.04)

# aggregate samples into a single corpus
scfile <- file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey", "JHU-DSS-capstone", "data", "sample_corpus_04.txt")
lapply(list(sample_twitter, sample_blog, sample_news), 
       function(x){write(x, file=scfile, append=TRUE)})

# clean up memory
rm(cblogs, cnews, ctwitter, 
   corpus_b, corpus_n, corpus_t, 
   sample_blog, sample_news, sample_twitter, 
   scfile, corpus_sample)

```

## Transformations and Tokenization
```{r}
scorp <- Corpus(DirSource(file.path("~", "Data Science Coursera", "capstone-JHU-swiftkey", "JHU-DSS-capstone", "data")))

# custom function to replace characters with spaces
to_space <- content_transformer(function(x, pattern) 
    str_replace_all(x, pattern, " "))

# replace /, @, |, -,  with a space
scorp <- tm_map(scorp, to_space, "/|@|\\||-")

# custom function to remove hashtags
rmv_hashtags <- content_transformer(function(x) 
    str_replace_all(x, "#.* |#.*$",""))
scorp <- tm_map(scorp, rmv_hashtags)

# make all words lowercase
scorp <- tm_map(scorp, content_transformer(tolower))

# remove numbers
scorp <- tm_map(scorp, removeNumbers)

# remove punctuation
scorp <- tm_map(scorp, removePunctuation)

# remove anything left that isn't letters
scorp <- tm_map(scorp, to_space, "[^A-Za-z]")

# strip whitespace
scorp <- tm_map(scorp, stripWhitespace)

# convert to plain text
scorp <- tm_map(scorp, PlainTextDocument)

# View: scorp[1][[1]][[1]][line#]
```

## Create Document Term Matrix

```{r}
dtm <- DocumentTermMatrix(scorp, control=list(wordLengths=c(0,Inf)))

# save to file when satisfied
saveRDS(dtm, "dtm.rds")
```

Create a word frequency list from the sampled and cleaned corpus.  

```{r}
freq <- colSums(as.matrix(dtm))
ord <- order(freq, decreasing=TRUE)

# plot 30 most frequent words
uni30 <- data.frame(word=names(freq[ord][1:30]), frequency=freq[ord][1:30]) %>% mutate(prop=frequency / sum(freq))

uniplot <- ggplot(uni30, aes(reorder(word, frequency), prop * 100))
uniplot <- uniplot + geom_bar(stat='identity')
uniplot <- uniplot + coord_flip()
```  

Visualize word frequencies. Specifically answer:  
1. What is the shape of the distribution of word frequencies?  
2. What top n-frequent words are needed to cover:  
    a. 50% of unique words?  
    b. 90% of unique words?  
    
```{r}
# create a plot of frequency of word-frequencies
word_freq_dist <- as.data.frame(table(freq)) %>% 
    rename(word_frequency = freq, frequency = Freq) %>%
    mutate(normed_frequency = frequency / sum(frequency),
           log_word_freq = log(as.numeric(word_frequency)),
           log_normed = log(normed_frequency))

# clean up, add trend line. Follows power-law
ggplot(data=word_freq_dist, aes(log_word_freq, log_normed)) + 
    geom_point(size=4, alpha=0.2)
```

Find n-frequent word cut-offs.  

```{r}
unique_words <- sum(word_freq_dist$frequency)
nfreq <- dim(word_freq_dist)[1]
pct_remaining <- numeric()
for (i in 1:nfreq - 1){
    j <- i + 1
    pct_remaining <- c(pct_remaining, 
                       sum(word_freq_dist[j:nfreq, 2]) / unique_words)
}
nfreq_removed <- data.frame(nfreq = as.numeric(word_freq_dist$word_frequency),
                            pct_remaining = pct_remaining)

# plot 
ggplot(data=nfreq_removed, aes(nfreq, pct_remaining)) + geom_line()
```  

Eliminating the words that appear only once narrows down the unique word library to 48%. However, many of the unique words are urls and foreign words.  Perform the same analysis after eliminating unique words.

```{r}
atleast_twice <- freq[freq > 1]
word_freq_dist2 <- as.data.frame(table(atleast_twice)) %>% 
    rename(word_frequency = atleast_twice, frequency = Freq) %>%
    mutate(normed_frequency = frequency / sum(frequency),
           log_word_freq = log(as.numeric(word_frequency)),
           log_normed = log(normed_frequency))

unique_words2 <- sum(word_freq_dist2$frequency)
nfreq2 <- dim(word_freq_dist2)[1]
pct_remaining2 <- numeric()
for (i in 1:nfreq2 - 1){
    j <- i + 1
    pct_remaining2 <- c(pct_remaining2, 
                       sum(word_freq_dist2[j:nfreq2, 2]) / unique_words2)
}
nfreq_removed2 <- data.frame(nfreq = as.numeric(as.character(word_freq_dist2$word_frequency)), pct_remaining = pct_remaining2)

ggplot(data=nfreq_removed2, aes(nfreq, pct_remaining)) + geom_line()
```

While the shape of the curve is the same, treating the 1-frequent words as nonwords leads to a different conclusion. Now, removing up to 5-frequent words still leaves more than half of all words in place.

## n-grams
n-grams are chains of n-word tokens. They can be used to build a language model, since the probability of a word following a small $n$ n-gram closely approximates the probability of a word following a large $n$ n-gram.

```{r}
# custom function for building n-grams
count_ngrams <- function(n) {
    options(mc.cores = 2)
    ctrl <- Weka_control(min = n, max = n)
    ngram_tokenizer <- function(x) NGramTokenizer(x, control = ctrl)
    dtm_ngram <- DocumentTermMatrix(scorp, 
                                    control = list(tokenize = ngram_tokenizer))
    return(dtm_ngram)
}

# build unigrams
unigrams <- count_ngrams(1)
saveRDS(unigrams, "unigrams.rds")

# build bigrams
bigrams <- count_ngrams(2)
saveRDS(bigrams, "bigrams.rds")

# build trigrams
trigrams <- count_ngrams(3)
saveRDS(trigrams, "trigrams.rds")
```

## Visualize most frequent uni-, bi-, and trigrams

```{r}
top30plot <- function(dtm) {
    freq <- colSums(as.matrix(dtm))
    ord <- order(freq, decreasing=TRUE)

    # plot 30 most frequent words
    df <- data.frame(word=names(freq[ord][1:30]), frequency=freq[ord][1:30])
    df <- mutate(df, prop=frequency / sum(freq))

    g <- ggplot(df, aes(reorder(word, frequency), prop * 100))
    g <- g + geom_bar(stat='identity')
    g <- g + coord_flip()
    return(g)
}

top30plot(bigrams)
top30plot(trigrams)
```

## Frequency Tables

NEED TO FIX: unigrams do not contain 1- or 2- letter words.

For prediction, a sorted frequency n-gram table is needed.
Format:

columnns: word_i  
rows: ngram word_{i - 1}  
entries: log probability of count.word_i / count.ngram word_{i - 1}

```{r}
unigrams <- readRDS('unigrams.rds')
bigrams <- readRDS('bigrams.rds')
trigrams <- readRDS('trigrams.rds')

freq_sorted <- function(ngram) {
    return(sort(col_sums(ngram), decreasing=TRUE))
}

unisorted <- freq_sorted(unigrams)
bisorted <- freq_sorted(bigrams)
trisorted <- freq_sorted(trigrams)

# example regex for subsetting bigrams by second word:
# second_word_of <- "(.* of$)"  # does "of$" work?
# result <- bisorted[str_detect(names(bisorted), second_word_of)]

unidat <- data.table(ngram = names(unisorted), frequency = unisorted)
bidat <- data.table(ngram = names(bisorted), frequency = bisorted)
tridat <- data.table(ngram = names(trisorted), frequency = trisorted)

# Save data.tables for shiny
saveRDS(unidat, "unidat.rds")
saveRDS(bidat, "bidat.rds")
saveRDS(tridat, "tridat.rds")
```

```{r}
# example data.table:
#    ngram  frequency  minus_i  last
# rock and         42    rock   and
####################################
#        ngram  frequency   minus_i  last
#rock and roll         18  rock and  roll

# Function to extract the ith word given the i-1 ngram
# If input ngram not found, returns NA (fix later)
extracti <- function(ngram, n) {
    pattern <- str_c("^", stri_trans_tolower(ngram))
    
    if (n == 2) {dat <- bidat}
    else if (n == 3) {dat <- tridat}
    else {dat <- tetradat}
    
    return(str_split(dplyr::filter(dat, 
                            str_detect(dat$ngram, pattern)
                            )$ngram[1], 
                     " ")[[1]][n]
           )   
}
```